
# AI-Powered Document Search Backend (RAG with FastAPI + Qdrant + OpenAI)

This is a backend-only project that powers an AI chatbot which can:

- Accept multiple PDF uploads
- Extract and chunk text
- Generate embeddings for each chunk
- Store embeddings in a vector database (Qdrant)
- Answer questions grounded in your documents (RAG: Retrieval-Augmented Generation)
- Return which file(s) the answer was derived from

You can later plug any frontend (Next.js, React, etc.) into this backend via the `/upload` and `/ask` endpoints.

---

## Tech Stack

- **Python 3.10+**
- **FastAPI** (web framework)
- **Qdrant** (vector database)
- **OpenAI** (embeddings + chat completions)
- **PyPDF** (PDF text extraction)
- **Uvicorn** (ASGI server)

---

## Project Structure

```bash
backend_rag_ai/
  main.py
  rag/
    __init__.py
    pdf_loader.py
    chunker.py
    embeddings.py
    vectorstore.py
    qa.py
  data/
    uploads/          # Uploaded PDFs are stored here
  .env.example
  requirements.txt
  README.md
```

---

## 1. Setup Python Environment

```bash
cd backend_rag_ai
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux / macOS:
source venv/bin/activate

pip install -r requirements.txt
```

---

## 2. Configure Environment Variables

Create a `.env` file based on `.env.example`:

```bash
cp .env.example .env
```

Fill values:

- `OPENAI_API_KEY` â€” your OpenAI API key
- `QDRANT_URL` â€” URL for your Qdrant instance (e.g. `http://localhost:6333`)
- `QDRANT_API_KEY` â€” only needed if you use a managed Qdrant cloud instance

---

## 3. Run Qdrant (Vector Database)

For local development using Docker:

```bash
docker run -p 6333:6333 qdrant/qdrant
```

This will start Qdrant on `http://localhost:6333`.

---

## 4. Run the FastAPI Backend

```bash
uvicorn main:app --reload --port 8000
```

Now OpenAPI docs are available at:

- http://localhost:8000/docs

---

## 5. Upload PDFs

Use the `/upload` endpoint to upload PDFs.

Example with `curl`:

```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@/path/to/your/file.pdf"
```

If successful, the response will look like:

```json
{
  "status": "ok",
  "file": "file.pdf",
  "chunks": 42
}
```

This will:

1. Save the PDF under `data/uploads/`
2. Extract text from the PDF
3. Chunk the text
4. Generate embeddings for each chunk
5. Store them in Qdrant with the file name as `source`

You can call `/upload` multiple times for multiple PDFs. All of them will be indexed into the same `docs` collection.

---

## 6. Ask Questions

Use the `/ask` endpoint to query your document knowledge base.

Example:

```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the sick leave policy?"}'
```

Example response:

```json
{
  "answer": "Employees are entitled to 12 days of sick leave per year...",
  "sources": [
    "Employee Handbook 2024.pdf",
    "Leave & Attendance Policy.pdf"
  ]
}
```

- `answer` is generated by the LLM using only the retrieved document chunks as context.
- `sources` is a list of file names used as the context for the answer.

---

## 7. Files Overview

### `main.py`

Defines FastAPI app and endpoints:

- `GET /health` â€” simple health check
- `POST /upload` â€” upload and index a single PDF
- `POST /ask` â€” ask a question; returns answer + source files

### `rag/pdf_loader.py`

Responsible for reading PDFs and extracting text.

### `rag/chunker.py`

Splits long text into overlapping chunks for better retrieval performance.

### `rag/embeddings.py`

Wraps OpenAI embeddings API.

### `rag/vectorstore.py`

Responsible for:

- Creating a Qdrant collection
- Adding new chunks with embeddings
- Searching for the most similar chunks

### `rag/qa.py`

Handles the RAG QA pipeline:

- Embed question
- Retrieve top chunks
- Compose context
- Call OpenAI ChatCompletion
- Return answer + sources

---

## 8. Notes

- This is a **backend-only** implementation ideal for pairing with a separate frontend (Next.js, React, etc.).
- All heavy AI work (embeddings + LLM) is done via OpenAI API.
- You can later:
  - Add user authentication
  - Add per-user / per-team collections
  - Add document deletion + reindex
  - Add feedback logging for answers

---

## 9. Quick Troubleshooting

- If you get Qdrant errors, ensure the container is running and `QDRANT_URL` is correct.
- If you get OpenAI auth errors, check your `OPENAI_API_KEY`.
- If PDF text seems empty, try another PDF (some scanned PDFs may need OCR, which is not included here yet).

Enjoy hacking on your AI Knowledge Base backend! ðŸš€
